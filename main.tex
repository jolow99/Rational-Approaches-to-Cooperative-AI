\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\title{Naive Utility Calculus}
\author{Joseph Low}
\date{August 2025}

\AtBeginSection[]{
  \begin{frame}
  \frametitle{Table of Contents}
  \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}
\frametitle{Introduction}
\vfill
\begin{center}
\fcolorbox{blue}{white}{\parbox{0.8\textwidth}{\centering\Large How do we make sense of other people's behavior?}}
\end{center}
\vfill

\pause

\begin{itemize}
    \item<2-> Why did you sleep late last night?
    \pause
    \item<3-> Why did you sign up for this course?
    \pause
    \item<4-> Why did you choose to eat out instead of cooking?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Utility-based decision-making}
\begin{center}
\Large $\text{Utility} = \text{Rewards} - \text{Costs}$
\end{center}
\vspace{0.5cm}
\begin{itemize}
    \item There is empirical support that humans intuitively use utility-based reasoning to make sense of other people's behavior
\end{itemize}
\pause

\vspace{0.5cm}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=0.8\textwidth]{utility1.png}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
\pause
\begin{itemize}
    \item Pursue high-cost implies reward was even higher
    \pause
    \item Pursue low-cost does not give info about reward
    \pause
    \item Forego high-cost does not give info about reward
    \pause
    \item Forego low-cost implies reward was even lower
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Naive Utility Calculus}
\begin{center}
\Large $U(p, o) = R(o) - C(p)$
\end{center}
\vspace{0.5cm}
\begin{itemize}
    \item $U(p, o)$: utility expected from acting according to plan $p$ to reach outcome $o$
    \item $R(o)$: subjective reward the agent expects from outcome $o$
    \item $C(p)$: subjective cost of executing plan $p$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Caveats}
\begin{itemize}
    \item \textbf{Descriptive, not normative:} This is not about how people \textit{should} make decisions (economic utility theory)
    \item \textbf{How we actually operate:} This describes how we \textit{intuitively} make sense of other people's behavior
    \item People don't explicitly compute utilities when they act - this is the cognitive framework we use to understand others
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Utility and Efficiency}
\begin{center}
\includegraphics[width=0.3\textwidth]{utility2.png}
\end{center}
\vspace{0.3cm}
\begin{itemize}
    \item More efficient paths are less costly and therefore produce higher utilities
    \item When agents act, they will fulfill their goals as efficiently as possible to maximize utility
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Graded Preference Inference}
\begin{center}
Does the agent prefer the green or purple star? To what extent? 
\end{center}
\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.7\textwidth]{utility3.png}
\end{center}

\vspace{0.3cm}
\begin{itemize}
    \pause
    \item \textbf{Fig.1: Pursue high-cost}: Agent pays high cost (long detour) to get purple star → strong preference inference
    \pause
    \item \textbf{Fig.2: Pursue low-cost}: Agent pays low cost (short path) to get purple star → weak preference inference
    \pause
    \item \textbf{Fig.3: Forego high-cost}: Agent foregoes high cost (doesn't climb wall) to get green star, chooses purple instead → no preference inference
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Costs and Rewards Vary Across Agents}
\begin{center}
What do the agent's actions imply about their preferences?
\end{center}
\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.7\textwidth]{utility4.png}
\end{center}

\vspace{0.3cm}
\begin{itemize}
    \pause
    \item \textbf{Costs vary}: Blue agent can climb walls easily, pink agent cannot → different action costs
    \pause
    \item \textbf{Rewards vary}: Blue agent prefers red stars, pink agent prefers green stars → different goal values
    \pause
    \item \textbf{Same action, different inference}: Identical behavior can imply different preferences based on agent capabilities and values
\end{itemize}
\end{frame}

\section{Related Work}
\begin{frame}
\frametitle{Feature-Based Approach}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{feature_based.png}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
    \item \textbf{Method}: Look at simple features of the choice and apply rules
    \item \textbf{Example}: Alice chooses \{eggplant sandwich\} over \{turkey, tuna, ham\}
    \item \textbf{Feature}: "She chose the only option with eggplant"
    \item \textbf{Rule}: "When someone picks the unique option, they probably like that thing"
    \item \textbf{Conclusion}: "Alice likes eggplant"
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Inverse Decision-Making Approach}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{inverse_decision.png}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
    \item \textbf{Method}: Use a model of how people actually make decisions, then work backwards
    \item \textbf{Example}: Same choice - Alice chooses \{eggplant sandwich\} over \{turkey, tuna, ham\}
    \item \textbf{Model thinking}: "If Alice really liked eggplant, how likely would she be to make this choice? What if she liked turkey instead? What if she just wanted any sandwich?"
    \item \textbf{Calculation}: Uses math to figure out which preference scenario makes this choice most probable
    \item \textbf{Conclusion}: "Alice probably likes eggplant" (but with precise confidence levels)
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{NUC vs. Inverse Decision-Making}
\begin{itemize}
    \item \textbf{Similarity}: NUC is similar to inverse decision-making as it works on assumption that agents maximize utilities
    \item \textbf{Key Differences}:
    \begin{itemize}
        \item Uses events with complex spatiotemporal structures and not just isolated discrete choices
        \item Computes both variables costs and rewards, whereas inverse decision-making only infers rewards without costs
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inverse Planning}
\begin{columns}
\begin{column}{0.5\textwidth}
\includegraphics[width=1.0\textwidth]{inverse_planning.png}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Belief-Desire Psychology ("Forward-thinking")}
\begin{itemize}
    \item Belief + Desire → Action
    \item "Sarah believes store is open + wants coffee → walks to coffee shop"
\end{itemize}

\vspace{0.4cm}
\textbf{Inverse Planning ("Backward-thinking")}
\begin{itemize}
    \item Observe: Environment + Action → Infer: Goal
    \item "See maze layout + this path → agent wants point A"
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Goal-Directed Action Understanding}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{goal_directed.png}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Limitations}
\begin{itemize}
    \item Model does not explain multiple causes behind other people's goals
    \item Treats cost as constant, observable and uniform across agents
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Research Questions}
\begin{itemize}
    \item \textbf{RQ1}: Can NUC support joint inference of costs and rewards when we know neither, using a coherent generative model?
    \item \textbf{RQ2}: Does NUC drive fine-grained quantitative inferences or only coarse qualitative ones?
    \item \textbf{RQ3}: Is NUC a unified generative model supporting probabilistic inference, or a collection of simple heuristics?
\end{itemize}
\end{frame}

\section{Experiments 1 and 2}

\begin{frame}
\frametitle{Interactive Experiment}
\begin{columns}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{experiment_1a-1.png}
\end{center}
\end{column}
\begin{column}{0.6\textwidth}
You will watch astronauts land on an alien planet with novel terrains and travel towards their space station. The astronauts can always travel across all types of terrain and can collect up to one care package on their way home.

\vspace{0.5cm}
\textbf{Your Task:} Determine the astronauts'
\begin{itemize}
    \item \textbf{Ability} to travel each kind of terrain (costs)
    \item \textbf{Desire} to collect each care package (rewards)
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Quick Check}
\begin{enumerate}
    \item Do astronauts have identical or different abilities?
    \vspace{0.5cm}
    \item Do astronauts have identical or different care package preferences?
    \vspace{0.5cm}
    \item Can astronauts always cross all terrains, even if exhausting?
    \vspace{0.5cm}
    \item How many care packages must/can astronauts collect?
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Quick Check}
\begin{enumerate}
    \item Do astronauts have identical or different abilities?
    \begin{itemize}
        \item[\textbf{Answer:}] Different astronauts have different abilities
    \end{itemize}
    \vspace{0.3cm}
    \item Do astronauts have identical or different care package preferences?
    \begin{itemize}
        \item[\textbf{Answer:}] Different astronauts have different preferences
    \end{itemize}
    \vspace{0.3cm}
    \item Can astronauts always cross all terrains, even if exhausting?
    \begin{itemize}
        \item[\textbf{Answer:}] Yes, all astronauts can travel across all terrains
    \end{itemize}
    \vspace{0.3cm}
    \item How many care packages must/can astronauts collect?
    \begin{itemize}
        \item[\textbf{Answer:}] Up to one care package (optional)
    \end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Questions to Answer}
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{For Terrain}\\
"How easy is it to cross this terrain?"

\vspace{0.3cm}
\textbf{Scale:} 0 to 10
\begin{itemize}
    \item \textbf{0} = Extremely easy
    \item \textbf{5} = Average  
    \item \textbf{10} = Extremely exhausting
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Care Package}\\
"How much does the astronaut like this container?"

\vspace{0.3cm}
\textbf{Scale:} 0 to 10
\begin{itemize}
    \item \textbf{0} = Not at all
    \item \textbf{5} = Average
    \item \textbf{10} = A lot
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
    \frametitle{Experiment 1a-1}
    \begin{columns}
    \begin{column}{0.5\textwidth}
    \begin{center}
    \includegraphics[width=0.9\textwidth]{experiment_1a-1.png}
    \end{center}
    \end{column}
    \begin{column}{0.5\textwidth}
    Based on this astronaut's path, rate on a scale of 0-10:
    
    \vspace{0.2cm}
    \textbf{1. Red terrain} \includegraphics[width=0.8cm]{red_terrain.png}\\
    How easy is it to cross?
    \vspace{0.2cm}
    
    \textbf{2. Blue terrain} \includegraphics[width=0.8cm]{blue_terrain.png}\\
    How easy is it to cross?
    \vspace{0.2cm}
    
    \textbf{3. White care package} \includegraphics[width=0.8cm]{white_carepackage.png}\\
    How much does the astronaut like it?
    \vspace{0.2cm}
    
    \textbf{4. Orange care package} \includegraphics[width=0.8cm]{orange_carepackage.png}\\
    How much does the astronaut like it?
    \end{column}
    \end{columns}
    \end{frame}

\begin{frame}
\frametitle{Experiment 1a-2}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=0.9\textwidth]{experiment_1a-2.png}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
Based on this astronaut's path, rate on a scale of 0-10:

\vspace{0.2cm}
\textbf{1. Red terrain} \includegraphics[width=0.8cm]{red_terrain.png}\\
How easy is it to cross?
\vspace{0.2cm}

\textbf{2. Blue terrain} \includegraphics[width=0.8cm]{blue_terrain.png}\\
How easy is it to cross?
\vspace{0.2cm}

\textbf{3. White care package} \includegraphics[width=0.8cm]{white_carepackage.png}\\
How much does the astronaut like it?
\vspace{0.2cm}

\textbf{4. Orange care package} \includegraphics[width=0.8cm]{orange_carepackage.png}\\
How much does the astronaut like it?
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Experiment 1b-1}
\begin{center}
\includegraphics[width=0.8\textwidth]{experiment_1b-1.png}
\end{center}
\vspace{0.3cm}
Based on this astronaut's path, rate on a scale of 0-10:
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{1. Blue terrain} \includegraphics[width=0.8cm]{blue_terrain.png}\\
How easy is it to cross?
\vspace{0.3cm}

\textbf{2. Yellow terrain} \includegraphics[width=0.8cm]{yellow_terrain.png}\\
How easy is it to cross?
\end{column}
\begin{column}{0.5\textwidth}
\textbf{3. White care package} \includegraphics[width=0.8cm]{white_carepackage.png}\\
How much does the astronaut like it?
\vspace{0.3cm}

\textbf{4. Orange care package} \includegraphics[width=0.8cm]{orange_carepackage.png}\\
How much does the astronaut like it?
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Experiment 1b-2}
\begin{center}
\includegraphics[width=0.8\textwidth]{experiment_1b-2.png}
\end{center}
\vspace{0.3cm}
Based on this astronaut's path, rate on a scale of 0-10:
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{1. Blue terrain} \includegraphics[width=0.8cm]{blue_terrain.png}\\
How easy is it to cross?
\vspace{0.3cm}

\textbf{2. Yellow terrain} \includegraphics[width=0.8cm]{yellow_terrain.png}\\
How easy is it to cross?
\end{column}
\begin{column}{0.5\textwidth}
\textbf{3. White care package} \includegraphics[width=0.8cm]{white_carepackage.png}\\
How much does the astronaut like it?
\vspace{0.3cm}

\textbf{4. Orange care package} \includegraphics[width=0.8cm]{orange_carepackage.png}\\
How much does the astronaut like it?
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Experiment 1c-1}
\begin{center}
\includegraphics[width=0.8\textwidth]{experiment_1c-1.png}
\end{center}
\vspace{0.3cm}
Based on this astronaut's path, rate on a scale of 0-10:
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{1. Red terrain} \includegraphics[width=0.8cm]{red_terrain.png}\\
How easy is it to cross?
\vspace{0.3cm}

\textbf{2. Yellow terrain} \includegraphics[width=0.8cm]{yellow_terrain.png}\\
How easy is it to cross?
\end{column}
\begin{column}{0.5\textwidth}
\textbf{3. White care package} \includegraphics[width=0.8cm]{white_carepackage.png}\\
How much does the astronaut like it?
\vspace{0.3cm}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Experiment 1c-2}
\begin{center}
\includegraphics[width=0.8\textwidth]{experiment_1c-2.png}
\end{center}
\vspace{0.3cm}
Based on this astronaut's path, rate on a scale of 0-10:
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{1. Red terrain} \includegraphics[width=0.8cm]{red_terrain.png}\\
How easy is it to cross?
\vspace{0.3cm}

\textbf{2. Yellow terrain} \includegraphics[width=0.8cm]{yellow_terrain.png}\\
How easy is it to cross?
\end{column}
\begin{column}{0.5\textwidth}
\textbf{3. White care package} \includegraphics[width=0.8cm]{white_carepackage.png}\\
How much does the astronaut like it?
\vspace{0.3cm}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Experiment Types}
Participants watched agents navigate worlds with different terrains and care packages to infer cost and reward functions. Three different geometries were used:

\vspace{0.3cm}
\begin{itemize}
    \item \textbf{Experiment 1a}: Detours could be explained by either terrain costs or reward differences
    
    \item \textbf{Experiment 1b}: Forced terrain crossings vs. optional detours to collect care packages
    
    \item \textbf{Experiment 1c}: Minimal physical detours that still reveal agent preferences
\end{itemize}

\vspace{0.3cm}
Responses evaluated against Naïve Utility Calculus model and two alternative models.
\end{frame}

\begin{frame}
\frametitle{Experiment 2}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=0.9\textwidth]{experiment2-1.png}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
Based on this astronaut's path across both maps, rate on a scale of 0-10:

\vspace{0.2cm}
\textbf{1. White terrain}\\
How easy is it to cross?
\vspace{0.2cm}

\textbf{2. Gray terrain}\\
How easy is it to cross?
\vspace{0.2cm}

\textbf{3. Reward A}\\
How much does the astronaut like it?
\vspace{0.2cm}

\textbf{4. Reward B}\\
How much does the astronaut like it?
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Why Experiment 2?}
Unlike Experiment 1's single events, real-world observations involve multiple actions across different contexts that let us revise beliefs about agents' costs and rewards.

\vspace{0.3cm}
Experiment 2 tests inference from multiple action events with two purposes:

\vspace{0.3cm}
\begin{itemize}
    \item Test if inferences over repeated events follow the full Naïve Utility Calculus model
    \item Test if a simpler single-event model (using first or most recent observation) explains inferences equally well
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Processing Human Data to "Ground Truth"}
\textbf{Step 1: Raw Data Collection}
\begin{itemize}
    \item Participants rated on 0-10 scales: terrain difficulty (0=easy, 10=exhausting) and care package preference (0=not at all, 10=a lot)
\end{itemize}

\vspace{0.2cm}
\textbf{Step 2: Z-Scoring Within Each Participant}
\begin{itemize}
    \item Remove individual scale usage differences by standardizing each participant's responses: z = (raw\_response - $\mu$\_participant) / $\sigma$\_participant
\end{itemize}

\vspace{0.2cm}
\textbf{Step 3: Separate by Question Type}
\begin{itemize}
    \item Split z-scored responses into cost judgments and reward judgments
\end{itemize}

\vspace{0.2cm}
\textbf{Step 4: Average Across Participants}
\begin{itemize}
    \item For each trial, average z-scores across all participants to create the "ground truth" score
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion Questions}
\begin{enumerate}
    \item What limitations are there to using humans as the "ground truth" here?
    \item Is it fair to isolate actions that result in costs and rewards? e.g. What changes if terrains have rewards or care packages have costs?
\end{enumerate}
\end{frame}

% \begin{frame}
% \frametitle{Experiment 5: Joint Cost-Reward Inference}
% \begin{itemize}
%     \item \textbf{Question}: Can people jointly infer costs and rewards from agent behavior?
%     \item \textbf{Design}: Agents with unknown capabilities and unknown goal preferences
%     \item \textbf{Manipulation}: Multiple trials revealing different cost-reward tradeoffs
%     \item \textbf{Results}:
%     \begin{itemize}
%         \item People successfully inferred both costs and rewards simultaneously
%         \item Judgments consistent with Bayesian model predictions
%         \item Performance improved with more observations
%     \end{itemize}
%     \item \textbf{Conclusion}: NUC supports complex joint inference in realistic scenarios
% \end{itemize}
% \end{frame}

\section{Naive Utility Calculus}

\begin{frame}
\frametitle{Key Question}
\begin{center}
\Large How do we design an algorithm that can give us the cost and reward function for each terrain and care package?
\end{center}

\vspace{0.5cm}
\textbf{The intuitive solution involves:}

\vspace{0.3cm}
\begin{enumerate}
    \item \textbf{Generation (Forward):} Given cost and reward → predict actions
    \item \textbf{Inference (Backward):} Given the actions → Infer the costs / rewards
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Generative model}
\begin{center}
\includegraphics[width=0.6\textwidth]{generative_model.png}
\end{center}

\vspace{0.5cm}
\begin{itemize}
    \pause
    \item \textbf{Desires} = Reward Function
    \pause
    \item \textbf{Goals} = States of the world that the agent believes yield rewards
    \pause
    \item \textbf{Intentions} = Ordered sequences of goals intended to maximize utilities
    \pause
    \item \textbf{Actions} = mappings from states to action which when executed sequentially fulfill each goal in the intention
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Generative Model}
\begin{center}
\includegraphics[width=0.6\textwidth]{generative_model2.png}
\end{center}

\vspace{0.3cm}
\begin{itemize}
    \pause
    \item \textbf{Beliefs + Rewards} → Candidate goals
    \pause
    \item \textbf{Goals + Action costs} → Specific intentions to act
    \pause
    \begin{itemize}
        \item Constraint: space station must be final goal
        \item Results in 5 possible intentions: direct path, single package routes, dual package routes
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion Question}
\begin{center}
\fcolorbox{blue}{white}{\parbox{0.9\textwidth}{\centering\Large Q1. The NUC model assumes that rational agents undergo a hierarchical planning process: First select an intention (a sequence/plan of goals), then take actions to achieve a goal. Would it be possible to model agents using a "flat" planning process, where agents directly optimize for subjective utility (rewards minus costs), without going through the process of selecting goals? If so, how would that generative model differ in its predictions?}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Markov Decision Processes (MDPs)}
\begin{columns}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[width=0.9\textwidth]{mdp.png}
\end{center}
\end{column}
\begin{column}{0.6\textwidth}
\textbf{Formal definition:} 4-tuple $(S, A, P_a, R_a)$
\begin{itemize}
    \item<2-> \textbf{States} (S): Agent positions
    \item<3-> \textbf{Actions} (A): 8 movement directions  
    \item<4-> \textbf{Transition probabilities} ($P_a$): Not important here (deterministic)
    \item<5-> \textbf{Rewards} ($R_a$): Only at goals
\end{itemize}

\vspace{0.3cm}
\textbf{In NUC context:}
\begin{itemize}
    \item<6-> Actions are deterministic
    \item<6-> Costs vary by terrain type
    \item<6-> Each goal has its own MDP
\end{itemize}
\end{column}
\end{columns}

\vspace{0.3cm}
\textbf{Purpose:} Compute optimal policies to reach goals efficiently, which observers use to infer agent preferences from observed behavior.
\end{frame}

\begin{frame}
\frametitle{Solving an MDP}
\begin{columns}
\begin{column}{0.3\textwidth}
\textbf{3x3 Grid Example:}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
 & & \\
\hline
\textbf{S} & \textbf{A} & \\
\hline
 & & \\
\hline
\end{tabular}
\end{center}
\end{column}
\begin{column}{0.65\textwidth}
\textbf{A} = Agent (center)\\
\textbf{S} = Space Station (goal)\\
\textbf{Optimal Solution:} Agent moves left
\end{column}
\end{columns}

\vspace{0.2cm}
\begin{columns}
\begin{column}{0.48\textwidth}
\textbf{Policy} $\pi(s)$:
$$\pi^*(s) = \arg\max_a Q^*(s,a)$$

\textbf{Value Function} $V(s)$:
$$V^*(s) = \max_a Q^*(s,a)$$
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Action-Value} $Q(s,a)$:
$$Q^*(s,a) = R + \gamma \sum_{s'} P V^*(s')$$

\textbf{Bellman Equation:}
$$V^*(s) = \max_a [R + \gamma \sum_{s'} P V^*(s')]$$
\end{column}
\end{columns}

\vspace{0.1cm}
{\tiny
\textbf{Optimal Policy $\pi^*$:} Maps each state to best action ("What should I do here?") $\bullet$ 
\textbf{Value Function $V^*$:} Expected reward from state with optimal policy ("How good is this state?") $\bullet$ 
\textbf{Action-Value $Q^*$:} Expected reward for specific action then optimal policy ("How good is this action?")
}
\end{frame}

\begin{frame}
\frametitle{Intentions to Action Policies}
\textbf{How do we "fulfill" an intention?}

\vspace{0.3cm}
\textbf{Answer:} Execute goal-specific MDPs for each goal until complete

\vspace{0.3cm}
\textbf{Example Intention:} Orange care package → Space station
\begin{itemize}
    \item \textbf{Step 1:} Solve MDP to reach orange care package
    \item \textbf{Step 2:} Solve MDP from orange package to space station
    \item Execute these policies sequentially
\end{itemize}

\vspace{0.3cm}
\textbf{Softmax Policy (Realistic Behavior):}
\begin{itemize}
    \item People don't always act perfectly - they make mistakes
    \item \textbf{Softmax:} Agents are more likely to choose good actions, less likely to choose bad ones
    \item $$p(a|s) \propto \exp(\alpha \cdot \text{Value of action})$$
    \item $\alpha = 0.1$ (low rationality parameter → nearly optimal behavior)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inference over the Generative Model}
\textbf{The Problem:} We observe actions, but need to infer hidden costs and rewards

\vspace{0.3cm}
\textbf{The Solution:} Bayesian inference - "work backwards" from actions
$$p(\text{Costs, Rewards}|\text{Actions}) \propto p(\text{Actions}|\text{Costs, Rewards}) \times p(\text{Costs, Rewards})$$

\vspace{0.3cm}
\textbf{How it works:}
\begin{itemize}
    \item \textbf{Prior:} What costs/rewards are plausible? (uniform for novel terrains)
    \item \textbf{Likelihood:} How probable are these actions given specific costs/rewards?
    \item \textbf{Posterior:} Which costs/rewards best explain the observed actions?
\end{itemize}

\vspace{0.3cm}
\textbf{Two levels of rationality:}
\begin{itemize}
    \item \textbf{Rational choice:} Pick intentions that maximize utility
    \item \textbf{Rational action:} Execute actions that fulfill intentions efficiently
\end{itemize}

\vspace{0.2cm}
\textbf{Implementation:} Monte Carlo sampling - test many cost/reward combinations and see which ones make the observed actions most likely
\end{frame}

\section{Experiment 5}

\begin{frame}
\frametitle{Recap: Research Questions}
\begin{itemize}
    \item \textbf{RQ1}: Can NUC support joint inference of costs and rewards when we know neither, using a coherent generative model?
    \item \textbf{RQ2}: Does NUC drive fine-grained quantitative inferences or only coarse qualitative ones?
    \item \textbf{RQ3}: Is NUC a unified generative model supporting probabilistic inference, or a collection of simple heuristics?
\end{itemize}

\vspace{0.5cm}
\textbf{Experiment 1:} Compare model predictions with human joint inferences of agent's costs and rewards

\vspace{0.3cm}
\textbf{Experiment 2:} Same comparison when multiple events are combined to make inferences

\vspace{0.3cm}
\textbf{Experiment 5:} Test how participants infer whether an agent is knowledgeable or ignorant (answers RQ3)
\end{frame}

\begin{frame}
\frametitle{Experiment 5: Knowledge vs. Ignorance}
\begin{center}
\includegraphics[width=0.65\textwidth]{experiment_5.png}
\end{center}

\vspace{0.15cm}
\textbf{Three Questions Asked:}
\begin{enumerate}
    \item \textbf{Attention check:} "Did the position of the care packages switch on the second day?" (Must answer correctly to proceed)
    
    \vspace{0.2cm}
    \item \textbf{Knowledge of rewards:} "Did the astronaut already know the rewards on day 1?"
    
    \vspace{0.2cm}
    \item \textbf{Knowledge of costs:} "Did the astronaut already know the costs on day 1?"
\end{enumerate}

\vspace{0.3cm}
\textbf{Response scale:} 0-10 ("Definitely did not" → "Maybe" → "Definitely did")
\end{frame}

\section{Results}

\begin{frame}
\frametitle{Alternative Models}
\textbf{1. NUC Variations:}
\begin{itemize}
    \item \textbf{Rate-based model:} $U = R/C$ (maximize reward rate) instead of $U = R - C$
    \item \textbf{Single-event model:} Use only first or most recent observation instead of integrating all events
    \item \textbf{Basic goal attribution:} Simple inverse planning without cost-benefit analysis
\end{itemize}

\vspace{0.15cm}
\textbf{2. Domain-Specific Heuristics:}
\begin{itemize}
    \item \textbf{Time-based:} Infer costs from time spent in each terrain
    \item \textbf{Collection-based:} Infer rewards from which objects are collected
    \item \textbf{Change-based:} Associate behavior changes with ignorance
    \item \textbf{Action-based:} Judge traits solely from specific actions (helping = nice)
\end{itemize}

\vspace{0.15cm}
\textbf{Goal:} Test if NUC captures fine-grained inferences that simpler alternatives miss
\end{frame}

\begin{frame}
\frametitle{Experiment 1 Results}
\begin{center}
\includegraphics[width=0.6\textwidth]{results_1.png}
\end{center}

\vspace{0.05cm}
\begin{itemize}
    \item \textbf{NUC model:} High correlations (0.85 costs, 0.95 rewards)
    \item \textbf{Rate model:} Lower correlations (0.70 costs, 0.87 rewards) 
    \item \textbf{Heuristic:} High overall correlation but missed fine-grained patterns
    \item \textbf{Conclusion:} People make graded inferences consistent with utility maximization, not just simple heuristics
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Experiment 2 Results}
\begin{center}
\includegraphics[width=0.6\textwidth]{results_2.png}
\end{center}

\vspace{0.05cm}
\begin{itemize}
    \item \textbf{Full NUC model:} High correlations (0.86 costs, 0.92 rewards)
    \item \textbf{First-only model:} Lower correlations (0.61 costs, 0.69 rewards)
    \item \textbf{Second-only model:} Lower correlations (0.57 costs, 0.68 rewards)
    \item \textbf{Conclusion:} People integrate information from multiple events for more precise inferences
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Experiment 5 Results}
\begin{center}
\includegraphics[width=0.4\textwidth]{results_5.png}
\end{center}

\vspace{0.05cm}
\begin{itemize}
    \item \textbf{NUC model:} High correlations (0.93 costs, 0.83 rewards)
    \item \textbf{Simple heuristic:} High overall correlation (0.97 costs, 0.99 rewards) but predicts only bimodal judgments
    \item \textbf{Graded sensitivity:} Participants showed nuanced inferences within heuristic clusters
    \item \textbf{Conclusion:} People make sophisticated inferences about agent knowledge, not just binary ignorant/knowledgeable judgments
\end{itemize}

\end{frame}


\section{Discussion}
\begin{frame}
\frametitle{Discussion}
\begin{itemize}
    \item Experiment 5 tests whether the NUC model can infer whether agents are initially ignorant or knowledgeable about their costs or rewards (i.e. their desires). However, the paper itself doesn't describe how knowledge/ignorance about one's own desires is accounted for in the generative model. How do you think the generative model presented in Section 2 could be modified to account for knowledge/ignorance of one's own rewards or costs?
\end{itemize}
\end{frame}

\end{document}